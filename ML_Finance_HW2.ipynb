{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df481f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1. Done\n",
      "\n",
      " 2. The difference between a Forward contract and a Futures contract is that a Forward contract is OTC and a Futures contract is exchange-traded.\n",
      "A Forward contract is a commitment to purchase at a future date given amount of a commodity or an asset at a price agreed on today.\n",
      "A Futures contract is an exchange-traded, standardized forward-like contract that is marked to market daily.\n",
      "74.62961831098825\n",
      "\n",
      " 3. The value of the forward contract is: $1685.19\n",
      "\n",
      " 4.\n",
      "Total revenue without hedge: $4500.0\n",
      "Total revenue with hedge: $5000.0\n",
      "The farmer has saved $500.0 by hedging.\n",
      "\n",
      " 5.\n",
      "Arbitrage opportunity exists. Strategy: Sell the forward contract, buy the commodity now.\n",
      "Arbitrage profit: $0.84\n"
     ]
    }
   ],
   "source": [
    "#Lauren Jachimczyk\n",
    "#Machine Learning in Finance Homework 2\n",
    "\n",
    "# 1. (5 points) Sign up for your project team.\n",
    "print(\"\\n 1. Done\")\n",
    "\n",
    "# 2. (15 points) What’s the difference between a Forward contract and a Futures contract?\n",
    "print(\"\\n 2. The difference between a Forward contract and a Futures contract is that a\"\n",
    "      \" Forward contract is OTC and a Futures contract is exchange-traded.\" \n",
    "      \"\\nA Forward contract is a commitment to purchase at a future date given amount of a commodity or an asset at a price agreed on today.\"\n",
    "      \"\\nA Futures contract is an exchange-traded, standardized forward-like contract that is marked to market daily.\")\n",
    "\n",
    "# 3. (10 points) A company enters into a forward contract to buy 500 barrels of oil in 6 months at\n",
    "# $75 per barrel. Three months later, the current forward price for oil is $78 per barrel, and the\n",
    "# risk-free interest rate is 2% per annum. What is the value of the forward contract at this point?\n",
    "\n",
    "def PV(Pf,rf,m):\n",
    "      PV=Pf/(1+rf)**(m/12)\n",
    "      return PV\n",
    "original_fwd_pv = PV(75,0.02,3) #3 months remaining\n",
    "print(original_fwd_pv)\n",
    "current_forward_price=78\n",
    "\n",
    "Vf = (current_forward_price-original_fwd_pv)*500 #500 barrels\n",
    "print(f\"\\n 3. The value of the forward contract is: ${Vf:.2f}\")\n",
    "\n",
    "# 4. (10 points) A wheat farmer expects to harvest 1,000 bushels of wheat in 6 months. To hedge\n",
    "# against the risk of a decline in wheat prices, the farmer sells 6-month wheat futures contracts at\n",
    "# $5 per bushel. If the spot price of wheat in 6 months is $4.50 per bushel, how much has the farmer\n",
    "# saved or lost by hedging?\n",
    "\n",
    "quantity = 1000\n",
    "futures_price = 5.0 \n",
    "spot_price_6mo = 4.5\n",
    "\n",
    "total_revenue_without_hedge = spot_price_6mo * quantity\n",
    "total_revenue_with_hedge = futures_price * quantity\n",
    "hedge_result = total_revenue_with_hedge - total_revenue_without_hedge\n",
    "\n",
    "print(\"\\n 4.\")\n",
    "print(f\"Total revenue without hedge: ${total_revenue_without_hedge}\")\n",
    "print(f\"Total revenue with hedge: ${total_revenue_with_hedge}\")\n",
    "print(f\"The farmer has {'saved' if hedge_result > 0 else 'lost'} ${abs(hedge_result)} by hedging.\")\n",
    "\n",
    "# 5. (10 points) Suppose the spot price of a commodity is $50, and the forward price for delivery in 1\n",
    "# year is $55. If the risk-free rate is 8%, is there an arbitrage opportunity? If so, how would you\n",
    "# exploit it?\n",
    "import math\n",
    "\n",
    "spot_price = 50 \n",
    "forward_price = 55 \n",
    "risk_free_rate = 0.08 \n",
    "T = 1  #years\n",
    "\n",
    "theoretical_forward_price = spot_price * math.exp(risk_free_rate * T)\n",
    "\n",
    "print(\"\\n 5.\")\n",
    "\n",
    "if theoretical_forward_price < forward_price:\n",
    "    print(\"Arbitrage opportunity exists. Strategy: Sell the forward contract, buy the commodity now.\")\n",
    "    arbitrage_profit = forward_price - theoretical_forward_price\n",
    "    print(f\"Arbitrage profit: ${arbitrage_profit:.2f}\")\n",
    "elif theoretical_forward_price > forward_price:\n",
    "    print(\"Arbitrage opportunity exists. Strategy: Buy the forward contract, short-sell the commodity now.\")\n",
    "    arbitrage_profit = theoretical_forward_price - forward_price\n",
    "    print(f\"Arbitrage profit: ${arbitrage_profit:.2f}\")\n",
    "else:\n",
    "    print(\"No arbitrage opportunity exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d3123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurenjachimczyk/anaconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Users/laurenjachimczyk/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.1263\n",
      "Epoch 2/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0059\n",
      "Epoch 3/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0050\n",
      "Epoch 4/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0043\n",
      "Epoch 5/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0050\n",
      "Epoch 6/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0048\n",
      "Epoch 7/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0044\n",
      "Epoch 8/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0043\n",
      "Epoch 9/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0040\n",
      "Epoch 10/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0044\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 632us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0043\n",
      "Epoch 2/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0042\n",
      "Epoch 3/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0041\n",
      "Epoch 4/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0042\n",
      "Epoch 5/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0044\n",
      "Epoch 6/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0040\n",
      "Epoch 7/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0042\n",
      "Epoch 8/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0040\n",
      "Epoch 9/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0045\n",
      "Epoch 10/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0040\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurenjachimczyk/anaconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 148.0983\n",
      "Epoch 2/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0120\n",
      "Epoch 3/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0107\n",
      "Epoch 4/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0086\n",
      "Epoch 5/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 0.0083\n",
      "Epoch 6/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0071\n",
      "Epoch 7/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0057\n",
      "Epoch 8/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0052\n",
      "Epoch 9/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0045\n",
      "Epoch 10/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0046\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 783us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0043\n",
      "Epoch 2/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 0.0047\n",
      "Epoch 3/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0043\n",
      "Epoch 4/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0044\n",
      "Epoch 5/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 0.0042\n",
      "Epoch 6/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0042\n",
      "Epoch 7/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 0.0042\n",
      "Epoch 8/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0039\n",
      "Epoch 9/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0043\n",
      "Epoch 10/10\n",
      "\u001b[1m3411/3411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "# 6. (50 points) The data file ”IR-data.xlsx” contain approximately 11 years of weekly interest rate\n",
    "# data. Use the data points prior to 2021 to train RNN and LSTM models, respectively, using\n",
    "# 50 data points as the lag window. Use the trained models to predict the interest rate for each\n",
    "# test data point in 2021 and beyond. For each model, report the following model parameters and\n",
    "# outcome:\n",
    "# (a) The model architecture summary output\n",
    "# (b) # of epochs trained and final training MSE loss\n",
    "# (c) For each predicted point, report the forecasted value\n",
    "# (d) The overall MSE and R2 for the test data\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "raw_data = pd.read_excel('IR-data.xlsx')\n",
    "raw_data['IR'] = pd.to_numeric(raw_data['IR'], errors='coerce')\n",
    "\n",
    "\n",
    "clean_data = raw_data.dropna(subset=['IR'])\n",
    "clean_data = clean_data.sort_values(by='DATE').reset_index(drop=True)\n",
    "clean_data['DATE'] = pd.to_datetime(clean_data['DATE'])\n",
    "# print(clean_data)\n",
    "\n",
    "#split into training and testing data\n",
    "#training data (prior to 2021)\n",
    "train_data = clean_data[clean_data['DATE'] < '2021-01-01']\n",
    "#testing data (2021 and beyond)\n",
    "test_data = clean_data[clean_data['DATE'] >= '2021-01-01']\n",
    "train_values = train_data['IR'].values\n",
    "test_values = test_data['IR'].values\n",
    "\n",
    "#create sequences\n",
    "seq_length = 50\n",
    "batch_size = 1\n",
    "\n",
    "train_generator = TimeseriesGenerator(train_values, train_values, length=seq_length, batch_size=batch_size)\n",
    "test_generator = TimeseriesGenerator(test_values, test_values, length=seq_length, batch_size=batch_size)\n",
    "\n",
    "#RNN model\n",
    "rnn_model = Sequential([\n",
    "    SimpleRNN(50, activation='relu', input_shape=(seq_length, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "rnn_model.compile(optimizer='adam', loss='mse')\n",
    "rnn_model.fit(train_generator, epochs=10, verbose=1)\n",
    "rnn_predictions = rnn_model.predict(test_generator).flatten()\n",
    "\n",
    "rnn_mse = mean_squared_error(test_values[seq_length:], rnn_predictions)\n",
    "rnn_r2 = r2_score(test_values[seq_length:], rnn_predictions)\n",
    "\n",
    "rnn_history = rnn_model.fit(train_generator, epochs=10, verbose=1)\n",
    "rnn_final_mse = rnn_history.history['loss'][-1] \n",
    "\n",
    "#LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(seq_length, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "lstm_model.fit(train_generator, epochs=10, verbose=1)\n",
    "lstm_predictions = lstm_model.predict(test_generator).flatten()\n",
    "\n",
    "lstm_mse = mean_squared_error(test_values[seq_length:], lstm_predictions)\n",
    "lstm_r2 = r2_score(test_values[seq_length:], lstm_predictions)\n",
    "\n",
    "lstm_history = lstm_model.fit(train_generator, epochs=10, verbose=1)\n",
    "lstm_final_mse = lstm_history.history['loss'][-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b0459cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 6.\n",
      "RNN \n",
      " A)Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m2,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,955</span> (31.08 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,955\u001b[0m (31.08 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,651</span> (10.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,651\u001b[0m (10.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,304</span> (20.72 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m5,304\u001b[0m (20.72 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " B)The number of epochs trained is 10 and the final training MSE loss is 0.0040\n",
      "\n",
      " C)Forecasted Values for Each Predicted Point:\n",
      "[1.6217276 1.620811  1.7060771 1.7347844 1.6885326 1.6192039 1.6122515\n",
      " 1.6200721 1.6635228 1.7275709 1.7274379 1.7371621 1.6798487 1.713481\n",
      " 1.7190197 1.6649742 1.6733599 1.627886  1.6653749 1.6811848 1.6358898\n",
      " 1.6336181 1.5464321 1.5868337 1.5989386 1.5768864 1.5643551 1.5623672\n",
      " 1.5733851 1.5744181 1.6268445 1.6244419 1.6482563 1.6426929 1.6243944\n",
      " 1.6018398 1.5816206 1.5722144 1.5935891 1.6250963 1.63654   1.6874832\n",
      " 1.6531676 1.6247487 1.6318243 1.6308148 1.6760302 1.622755  1.6261805\n",
      " 1.599193  1.5512173 1.5750214 1.6010096 1.5761418 1.6171517 1.580914\n",
      " 1.6281    1.5487258 1.5664747 1.5195343 1.492409  1.4411199 1.4664265\n",
      " 1.502456  1.507267  1.5694337 1.5131438 1.4420539 1.4989586 1.4676727\n",
      " 1.4991405 1.4826155 1.5389047 1.4817369 1.4871562 1.4408472 1.4772983\n",
      " 1.4298615 1.3624396 1.3242912 1.2932289 1.3688519 1.3714473 1.4238436\n",
      " 1.3632412 1.3031842 1.3074974 1.1762159 1.2316973 1.295426  1.2663977\n",
      " 1.300645  1.2856318 1.2444346 1.259059  1.2767081 1.232972  1.196184\n",
      " 1.1874626 1.1861775 1.2290349 1.3083907 1.3307875 1.362803  1.3453612\n",
      " 1.3575143 1.2813113 1.2564194 1.256047  1.2654859 1.233777  1.2595451\n",
      " 1.2453346 1.2899387 1.3472693 1.3394334 1.3063128 1.286058  1.296807\n",
      " 1.3056791 1.2844718 1.3302097 1.3760664 1.3484945 1.2933977 1.3514892\n",
      " 1.3212855 1.2740974 1.3100045 1.3353845 1.3678269 1.3049402 1.3289874\n",
      " 1.3144717 1.4098547 1.4661598 1.4840602 1.5396366 1.5451545 1.5164362\n",
      " 1.4710006 1.4859413 1.5332708 1.525465  1.5800416 1.6043355 1.587472\n",
      " 1.5527761 1.5109476 1.5871322 1.5799546 1.652826  1.6432843 1.6795404\n",
      " 1.6508846 1.6349514 1.6202592 1.5280876 1.5669903 1.5380383 1.5779915\n",
      " 1.5519054 1.5987786 1.5196567 1.4402518 1.5098672 1.4453151 1.5628139\n",
      " 1.5716112 1.6356167 1.6232617 1.596461  1.5808358 1.5303419 1.627532\n",
      " 1.6616294 1.6416817 1.4645008 1.5195208 1.4127231 1.4250493 1.4329896\n",
      " 1.3400173 1.4325374 1.4705935 1.5236125 1.4865129 1.4753374 1.4100888\n",
      " 1.4373708 1.4624988 1.4357017 1.4040848 1.4272141 1.4743794 1.4560575\n",
      " 1.4998608 1.472427  1.4877925 1.5447253 1.515346  1.5180283 1.6259197\n",
      " 1.6557866 1.7137575 1.7250781 1.757201  1.7729385 1.7437074 1.7326732\n",
      " 1.6888022 1.774583  1.8634958 1.8295506 1.8274214 1.7340069 1.7420057\n",
      " 1.7678938 1.8464175 1.8046023 1.7767196 1.7773485 1.800856  1.7723447\n",
      " 1.8157554 1.9237521 1.9161532 1.9602042 1.9283276 2.0261917 1.9049335\n",
      " 1.9797051 2.0331922 2.027972  1.962045  1.906673  1.9267418 1.9791248\n",
      " 1.9538625 1.9656751 1.8129848 1.7035692 1.8493164 1.840773  1.7410687\n",
      " 1.7703564 1.8480216 1.9382885 1.9798207 1.9981986 2.1361158 2.1413875\n",
      " 2.1907885 2.188123  2.1301997 2.2974803 2.369159  2.3193605 2.3304675\n",
      " 2.4566958 2.450115  2.404039  2.3323352 2.3015509 2.374308  2.4089537\n",
      " 2.5378907 2.602207  2.6569865 2.7100086 2.7798738 2.7057164 2.687489\n",
      " 2.80023   2.8368433 2.925246  2.8329208 2.8895378 2.8751414 2.7958362\n",
      " 2.7506056 2.799699  2.8337498 2.8809028 2.9798028 2.956382  2.917677\n",
      " 3.026152  3.098407  3.0396264 2.9731467 2.882756  2.8151753 2.899802\n",
      " 2.8613975 2.9739845 2.8673158 2.8293242 2.7514791 2.8365686 2.7380304\n",
      " 2.7410328 2.7260554 2.7256293 2.83217   2.9262702 2.912129  2.9491699\n",
      " 3.0221508 2.9638524 3.0186293 3.0178375 3.1329525 3.3694718 3.4931872\n",
      " 3.3150167 3.257159  3.2111204 3.2762032 3.1364782 3.0736434 3.100066\n",
      " 3.171028  3.1874344 3.0831177 2.956458  2.8486316 2.7923648 2.893594\n",
      " 2.9882693 3.085501  2.9743283 2.9445121 2.879899  2.94057   2.909619\n",
      " 2.9504638 2.9922183 3.0274308 2.8913965 2.749097  2.7823112 2.785825\n",
      " 2.7696917 2.6626852 2.6524186 2.5745528 2.720826  2.713197  2.6744525\n",
      " 2.8075755 2.7506888 2.795068  2.7572296 2.8586817 2.8209465 2.7802157\n",
      " 2.8002992 2.8715837 2.866855  2.9722369 3.014285  3.0413268 3.0957057\n",
      " 3.0108664 3.0249124 3.093421  3.0952291 3.1384232 3.2399354 3.183424\n",
      " 3.3161395 3.243779  3.2803156 3.304218  3.355139  3.4036863 3.393285\n",
      " 3.4333026 3.427516  3.473152  3.5482438 3.4912317 3.6667945 3.6651556\n",
      " 3.8538268 3.9412873 3.700382  3.7367277 3.7697444 3.6503525 3.5960174\n",
      " 3.7067182 3.793475  3.878569  3.9103076 3.888479  3.9472268 3.9738317\n",
      " 4.000436  3.9860857 4.103684  4.201682  4.192198  4.227215  4.0643682\n",
      " 4.012071  3.9182231 3.9802277 4.057315  4.051698  4.0780993 4.112165\n",
      " 4.1464386 4.197768  4.113226  4.0951977 3.7764962 3.8461406 3.7510855\n",
      " 3.6521728 3.7342675 3.7851748 3.8157303 3.7359393 3.684674  3.6480544\n",
      " 3.6643453 3.7231407 3.6587746 3.5060341 3.47933   3.550132  3.4892967\n",
      " 3.4004157 3.4539173 3.531515  3.5982888 3.4898915 3.4698415 3.4061604\n",
      " 3.4566357 3.5377672 3.6651518 3.6646698 3.652887  3.7238832 3.8069746\n",
      " 3.865279  3.809117  3.8591232 3.7572062 3.666653  3.6781116 3.5173461\n",
      " 3.5087805 3.5676575 3.520161  3.408866  3.4647639 3.4986882 3.3511965\n",
      " 3.370548  3.437967  3.5022907 3.4425318 3.4415042 3.463226  3.5005252\n",
      " 3.5320618 3.5009239 3.3659012 3.3746898 3.4780822 3.5987904 3.6589198\n",
      " 3.6114848 3.6483753 3.709058  3.7009664 3.7523296 3.7867959 3.8418314\n",
      " 3.7971814 3.9199998 3.902774  3.8620284 3.922491  3.8915167 3.9006145\n",
      " 3.9774187 4.051889  3.9475381 3.9570482 3.934255  3.9570038 3.9026353\n",
      " 3.6702747 3.5149875 3.5784009 3.479301  3.5460937 3.3560135 3.4545557\n",
      " 3.5381324 3.468135  3.358659  3.3500247 3.4757316 3.5327418 3.5577111\n",
      " 3.5290048 3.4548569 3.4036891 3.3203592 3.2753425 3.2749946 3.3616707\n",
      " 3.3939228 3.4166245 3.3894322 3.4305544 3.4959848 3.5814433 3.5633905\n",
      " 3.583216  3.5125747 3.5494633 3.4914577 3.378482  3.4044363 3.487419\n",
      " 3.4229972 3.5690877 3.4091299 3.3656328 3.3339283 3.4075534 3.4941738\n",
      " 3.516747  3.4100745 3.3653858 3.42398   3.474612  3.5267165 3.5529592\n",
      " 3.6333966 3.6806557 3.7037044 3.6787424 3.7082083 3.7959569 3.780142\n",
      " 3.6681302 3.6108096 3.5766063 3.6529012 3.6690674 3.6838005 3.766353\n",
      " 3.705969  3.7325218 3.6998646 3.8076658 3.8060198 3.7001975 3.7445424\n",
      " 3.7071142 3.7013257 3.7721374 3.7165039 3.700661  3.7425373 3.6846013\n",
      " 3.8171728 3.7833285 3.8467014 3.917497  4.0224767 4.0414076 3.9877427\n",
      " 3.9617784 3.8250232 3.7299812 3.7853854 3.7833006 3.7817452 3.724128\n",
      " 3.819278  3.8133717 3.8437655 3.8858864 3.8363895 3.9737694 3.9319375\n",
      " 3.9541433 4.015907  4.058141  4.170455  4.023021  4.0698695 3.9780138\n",
      " 3.977959  4.048311  4.1280656 4.171457  4.1874695 4.253934  4.2740517\n",
      " 4.2358317 4.309325  4.309927  4.1630993 4.2012415 4.207551  4.1774497\n",
      " 4.0906816 4.0903654 4.056319  4.1430855 4.232052  4.2823086 4.2451944\n",
      " 4.2324686 4.258941  4.2416534 4.224365  4.2625065 4.300993  4.2958527\n",
      " 4.345954  4.320545  4.4479504 4.410308  4.523222  4.5268936 4.5892696\n",
      " 4.5586643 4.5634165 4.643903  4.7590547 4.7047377 4.6917534 4.7321196\n",
      " 4.6254096 4.548732  4.637664  4.596641  4.6818867 4.77599   4.876565\n",
      " 4.9533677 4.8991375 4.826812  4.789706  4.8845882 4.827757  4.812052\n",
      " 4.835705  4.8467674 4.737094  4.634081  4.526326  4.607568  4.5448127\n",
      " 4.462983  4.563489  4.5737686 4.6085644 4.4022174 4.4926434 4.403796\n",
      " 4.418821  4.3835554 4.3833966 4.3894033 4.4383197 4.360539  4.3122654\n",
      " 4.2320127 4.3204017 4.185808  4.2596817 4.13819   4.0971828 4.104397\n",
      " 4.1859403 4.208091  4.1769724 4.0072403 3.8845165 3.870959  3.9064653\n",
      " 3.909354  3.83626   3.8650362 3.8697155 3.8690403 3.7628758 3.8166912\n",
      " 3.8461294 3.9272463 3.8875978 3.972226  4.01991   3.990564  3.9968812\n",
      " 4.011692  3.9533288 3.9347832 4.0269895 4.073858  4.122228  4.1257405\n",
      " 4.0842295 4.1134214 4.148725  4.116036  4.12591   4.0483127 4.033412\n",
      " 3.9559438 3.840525  3.9749467 4.1089296 4.0766077 4.0671062 4.113359\n",
      " 4.143585  4.1478543 4.27113   4.244297  4.2184124 4.2658105 4.2398243\n",
      " 4.29667   4.300641  4.2348127 4.2516613 4.2756734 4.244104  4.2234197\n",
      " 4.1579075 4.191966  4.0961823 4.086007  4.0557413 4.0645437 4.0728664\n",
      " 4.133648  4.1663804 4.262214  4.2863264 4.318922  4.270717  4.2420263\n",
      " 4.237892  4.189451  4.224041  4.2097507 4.1751976 4.1712646 4.281314\n",
      " 4.332304  4.339424  4.280969  4.354255  4.384115  4.3362007 4.498166\n",
      " 4.527562  4.4790454 4.585447  4.6290708 4.5661016 4.610422  4.5830927\n",
      " 4.5937924 4.5772247 4.619042  4.6646304 4.642622  4.6000175 4.6502953\n",
      " 4.5955086 4.5515866 4.462041  4.456643  4.434468  4.4526925 4.419609\n",
      " 4.471716  4.448267  4.423893  4.325383  4.349449  4.37782   4.4155617\n",
      " 4.3828125 4.403635  4.437339  4.433344  4.5083685 4.57408   4.5246744\n",
      " 4.4807963 4.3709145 4.2945404 4.252638  4.248345  4.3763084 4.4410944\n",
      " 4.36724   4.27849   4.200053  4.1650443 4.2331524 4.193496  4.2282715\n",
      " 4.21836   4.2251325 4.201438  4.284355  4.2618065 4.338088  4.434851\n",
      " 4.40799   4.33314   4.2430553 4.244981  4.261896  4.2551274 4.171861\n",
      " 4.1504107 4.190715  4.142267  4.1358566 4.170036  4.220536  4.237332\n",
      " 4.225533  4.2535768 4.240764  4.17264   4.13997   4.1173244 4.060199\n",
      " 3.9596837 3.7642038 3.7465632 3.8357084 3.927131  3.9745445 3.9165673\n",
      " 3.8728983 3.8175664 3.801884  3.8816774 3.8675764 3.8400946 3.7912028\n",
      " 3.7618363 3.8263109 3.7848995 3.801368  3.8022532 3.8179018 3.8472733\n",
      " 3.888604  3.8160427 3.7454655]\n",
      "\n",
      " D) Overall MSE: {rnn_mse:.4f}, R2: {rnn_r2:.4f}\n",
      "\n",
      "LSTM \n",
      " A)Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m10,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,355</span> (122.48 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,355\u001b[0m (122.48 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,451</span> (40.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,451\u001b[0m (40.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,904</span> (81.66 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m20,904\u001b[0m (81.66 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " B)The number of epochs trained is 10 and the final training MSE loss is 0.0040 \n",
      "\n",
      " C)\n",
      "[1.6165215 1.6267232 1.6885164 1.7222354 1.6911684 1.639911  1.6237317\n",
      " 1.6286366 1.6623594 1.7134335 1.72235   1.735008  1.695957  1.7155288\n",
      " 1.725782  1.6783954 1.6798526 1.6459346 1.6657243 1.6843948 1.6467701\n",
      " 1.6413921 1.5746073 1.5881091 1.604997  1.584104  1.5731279 1.5707198\n",
      " 1.5781889 1.5794395 1.6198003 1.6258874 1.6448168 1.6480393 1.632787\n",
      " 1.6141741 1.5946413 1.5830914 1.5970973 1.6236792 1.6363325 1.6783574\n",
      " 1.6602882 1.6348909 1.6388001 1.6390151 1.672365  1.6363614 1.6316833\n",
      " 1.6135805 1.570215  1.5790441 1.603871  1.5839567 1.6140233 1.5935456\n",
      " 1.6237948 1.570996  1.5714357 1.5379796 1.5087422 1.4634385 1.4708264\n",
      " 1.5034606 1.5087821 1.5566887 1.5244619 1.4661169 1.4954209 1.4830812\n",
      " 1.4981331 1.4925523 1.5316563 1.4977185 1.4933915 1.4603026 1.478392\n",
      " 1.4491628 1.3902758 1.3476992 1.3151839 1.3616915 1.376919  1.4130466\n",
      " 1.3805552 1.3297819 1.3192358 1.2278898 1.2371702 1.2881259 1.2752889\n",
      " 1.2990838 1.2954677 1.2655641 1.2673335 1.2824985 1.2549073 1.22003\n",
      " 1.2046959 1.2008198 1.230391  1.2909783 1.3199024 1.3518057 1.3515801\n",
      " 1.3614357 1.3101652 1.2779653 1.2703161 1.2760013 1.2541844 1.265455\n",
      " 1.2590119 1.2892392 1.337317  1.3396187 1.3197858 1.3015045 1.3055334\n",
      " 1.3137752 1.3000468 1.3290482 1.3705266 1.3554851 1.3166909 1.3481665\n",
      " 1.3369514 1.2979982 1.3133297 1.3384794 1.3667958 1.3260409 1.3344747\n",
      " 1.3271176 1.3905091 1.4444174 1.4674915 1.5190805 1.5388881 1.5225992\n",
      " 1.4899502 1.4923109 1.5311216 1.5296686 1.5700479 1.6006776 1.591128\n",
      " 1.5666825 1.5304197 1.5780731 1.5849946 1.6344309 1.6429071 1.6721576\n",
      " 1.6607577 1.6436526 1.6325275 1.5578072 1.569508  1.5535924 1.5758379\n",
      " 1.5629529 1.5940102 1.5417831 1.4707482 1.5047593 1.4688871 1.5381646\n",
      " 1.5657576 1.6125773 1.621925  1.6024833 1.5922227 1.550154  1.6113745\n",
      " 1.6511948 1.637555  1.5112588 1.5208389 1.4491531 1.4371302 1.441348\n",
      " 1.3726834 1.4214418 1.4657469 1.505941  1.4909947 1.4828722 1.4344463\n",
      " 1.4415654 1.4659439 1.4466614 1.4205667 1.4309844 1.47113   1.462124\n",
      " 1.4945036 1.4833989 1.4904423 1.5369788 1.5210238 1.5209957 1.6004096\n",
      " 1.6367145 1.6860408 1.7138321 1.7465296 1.7707584 1.7515019 1.7414265\n",
      " 1.7063146 1.7631054 1.8372744 1.8192923 1.8242625 1.7585447 1.7504946\n",
      " 1.7733562 1.8307949 1.8064631 1.7817005 1.7865238 1.803864  1.7813752\n",
      " 1.8125236 1.8984511 1.9037281 1.9443626 1.9342532 2.005131  1.9200954\n",
      " 1.9661368 2.0253413 2.0199594 1.9720563 1.9237403 1.9348948 1.9774908\n",
      " 1.9575448 1.9655929 1.8459452 1.7392023 1.830044  1.8425548 1.7514526\n",
      " 1.7736914 1.8371291 1.9078057 1.9530299 1.9832867 2.0959377 2.1202202\n",
      " 2.1671329 2.18635   2.139793  2.275986  2.3349566 2.3048742 2.330172\n",
      " 2.4433572 2.4386873 2.405488  2.349037  2.3179476 2.3753583 2.4081762\n",
      " 2.5100815 2.5759158 2.6323211 2.6932337 2.7612295 2.7063985 2.6940174\n",
      " 2.8020332 2.8279903 2.908258  2.8361177 2.8884578 2.8905275 2.807319\n",
      " 2.7660654 2.8114395 2.8430083 2.8835998 2.9701025 2.9547238 2.9236555\n",
      " 3.0264506 3.090502  3.0352948 2.9829426 2.9037228 2.8347917 2.9140828\n",
      " 2.8679354 2.9609647 2.8742723 2.8332276 2.772985  2.8446496 2.7499325\n",
      " 2.7436852 2.7423482 2.734124  2.8282912 2.9086592 2.9025586 2.9480247\n",
      " 3.0195138 2.966441  3.0187235 3.0303586 3.1296842 3.3664825 3.4262273\n",
      " 3.302847  3.268644  3.2427971 3.301371  3.1482751 3.083386  3.1229236\n",
      " 3.1885507 3.1901863 3.0929573 2.9716747 2.8714855 2.8123708 2.9096\n",
      " 2.9832995 3.063406  2.9726696 2.9508421 2.9015608 2.9505541 2.9222808\n",
      " 2.954548  3.0008984 3.03241   2.9051728 2.7651823 2.8009582 2.8029978\n",
      " 2.7760563 2.6771352 2.663627  2.5949867 2.719557  2.7076168 2.6728473\n",
      " 2.7971437 2.7474189 2.7890298 2.7716007 2.8516335 2.8262236 2.7847888\n",
      " 2.8130515 2.8759484 2.8699694 2.959976  3.0070846 3.037525  3.0955193\n",
      " 3.0190237 3.0343523 3.1050282 3.0985062 3.1425076 3.237923  3.1817136\n",
      " 3.3048494 3.245286  3.2825005 3.3202684 3.3609383 3.4086301 3.4006848\n",
      " 3.4427423 3.442206  3.4835668 3.554465  3.4936244 3.669636  3.6555617\n",
      " 3.847567  3.9173064 3.6808066 3.74257   3.807767  3.645823  3.6093168\n",
      " 3.7379706 3.798682  3.8664212 3.907275  3.8947008 3.956425  3.9819522\n",
      " 4.0078354 3.9987807 4.120116  4.2012677 4.183186  4.2343254 4.066686\n",
      " 4.0245214 3.9429839 4.008642  4.0784893 4.04909   4.0882845 4.123361\n",
      " 4.15397   4.201797  4.1145897 4.1078906 3.7837365 3.8628333 3.7768826\n",
      " 3.6554668 3.75354   3.7980247 3.819098  3.7461078 3.697174  3.668849\n",
      " 3.6830692 3.740157  3.6633449 3.512927  3.4986572 3.5845501 3.4898796\n",
      " 3.4081464 3.4677293 3.5501006 3.5910568 3.4939516 3.480248  3.4296885\n",
      " 3.473034  3.5538645 3.6606472 3.656597  3.6618774 3.7351766 3.814467\n",
      " 3.8560944 3.8131807 3.8679087 3.7700326 3.6764379 3.703156  3.5343215\n",
      " 3.5219831 3.596884  3.523082  3.4174001 3.4779258 3.5172782 3.3549073\n",
      " 3.3819427 3.4635491 3.5031579 3.4478416 3.4521835 3.4832602 3.514171\n",
      " 3.5441818 3.51223   3.3780587 3.3925645 3.5090044 3.5971992 3.644591\n",
      " 3.613868  3.6593297 3.722617  3.7049627 3.7594619 3.7948673 3.8455207\n",
      " 3.804276  3.9292798 3.9020565 3.8701303 3.936062  3.902203  3.913731\n",
      " 3.9951386 4.0546503 3.9429314 3.969444  3.9589882 3.974597  3.918477\n",
      " 3.6738136 3.528117  3.6224325 3.4859955 3.5507216 3.3686512 3.4554796\n",
      " 3.560271  3.4536817 3.3657136 3.369327  3.50525   3.5244894 3.5606976\n",
      " 3.5394852 3.4700167 3.42048   3.3399086 3.29167   3.2935472 3.3763568\n",
      " 3.3966448 3.4246664 3.4026287 3.4428437 3.5059745 3.5788698 3.563238\n",
      " 3.592865  3.5295584 3.5641532 3.5105715 3.3893201 3.4240427 3.5144184\n",
      " 3.4205568 3.5647974 3.4050024 3.3687267 3.3600543 3.4288611 3.500571\n",
      " 3.5161512 3.4188707 3.380503  3.4472184 3.4860506 3.5294707 3.5600564\n",
      " 3.6345794 3.6797462 3.7064176 3.690218  3.7229187 3.8121219 3.7795303\n",
      " 3.6763422 3.628587  3.6014676 3.6783564 3.6759033 3.6949494 3.7755542\n",
      " 3.7089872 3.74198   3.719423  3.823346  3.8079188 3.7058663 3.7582715\n",
      " 3.7261112 3.7129633 3.787046  3.720483  3.711508  3.759576  3.6952755\n",
      " 3.828613  3.779948  3.8469198 3.9245827 4.0188007 4.032776  3.994534\n",
      " 3.9795544 3.841547  3.7439382 3.8167572 3.7942026 3.7934756 3.7384222\n",
      " 3.8340328 3.8189273 3.852069  3.896907  3.8437586 3.9865377 3.9250894\n",
      " 3.9599812 4.0307493 4.058964  4.1746225 4.0069613 4.07328   3.99468\n",
      " 3.9915576 4.0744867 4.134142  4.1676393 4.1919565 4.259199  4.2755103\n",
      " 4.242271  4.321658  4.315333  4.1639824 4.2138734 4.2318306 4.1837993\n",
      " 4.1012526 4.109184  4.077576  4.164557  4.239772  4.272207  4.2492127\n",
      " 4.2473874 4.2770042 4.2534633 4.23788   4.2767115 4.3111334 4.302699\n",
      " 4.354319  4.3275347 4.466054  4.396368  4.5255384 4.518422  4.5862036\n",
      " 4.5586786 4.5734143 4.662505  4.765164  4.6804104 4.697537  4.750249\n",
      " 4.615073  4.5536695 4.6733837 4.5830655 4.6859264 4.781916  4.8597717\n",
      " 4.9289193 4.883803  4.831124  4.805604  4.9218245 4.8022056 4.8158517\n",
      " 4.848773  4.8474245 4.730896  4.636198  4.5381722 4.6456017 4.5326834\n",
      " 4.464534  4.592355  4.570468  4.6110983 4.3898215 4.503634  4.405481\n",
      " 4.4260044 4.400596  4.3966517 4.4065413 4.45266   4.359033  4.3207088\n",
      " 4.248176  4.350119  4.174957  4.2608104 4.1438203 4.1039896 4.127953\n",
      " 4.212619  4.206926  4.1866207 4.011691  3.8964477 3.8979628 3.94128\n",
      " 3.9176385 3.8457344 3.8803957 3.8903573 3.8815029 3.7735577 3.828298\n",
      " 3.8651607 3.9337816 3.8894162 3.973545  4.0237126 3.9906669 4.0106134\n",
      " 4.030308  3.9659262 3.9500823 4.0523524 4.0746107 4.1235247 4.1326337\n",
      " 4.096302  4.1287365 4.1646605 4.122447  4.1404853 4.0626736 4.049106\n",
      " 3.9742646 3.8507307 4.0057516 4.123338  4.0531073 4.0769396 4.132527\n",
      " 4.1514325 4.158388  4.2877173 4.2334905 4.2278767 4.2833514 4.248186\n",
      " 4.305182  4.3091803 4.241287  4.2669287 4.2943063 4.2515764 4.2370915\n",
      " 4.172522  4.207933  4.1065063 4.0979333 4.0773864 4.081072  4.091533\n",
      " 4.147388  4.171519  4.267425  4.2799797 4.3207045 4.2766604 4.256255\n",
      " 4.259136  4.205139  4.2383556 4.2248244 4.1863036 4.1891813 4.309306\n",
      " 4.3261056 4.3417654 4.2890244 4.370824  4.3910103 4.3374143 4.520549\n",
      " 4.509584  4.4787955 4.6026874 4.6259227 4.5581646 4.6170506 4.590725\n",
      " 4.6036797 4.590081  4.6313157 4.67068   4.641073  4.6091237 4.6676164\n",
      " 4.594176  4.5589757 4.4733176 4.474664  4.4538245 4.466428  4.430873\n",
      " 4.4822593 4.454519  4.433245  4.334467  4.364465  4.4007    4.420144\n",
      " 4.389993  4.4145374 4.4496417 4.439519  4.519528  4.574183  4.516394\n",
      " 4.490558  4.383036  4.307383  4.27296   4.2680626 4.407975  4.42998\n",
      " 4.362313  4.285564  4.2187753 4.1841707 4.2637444 4.191171  4.236376\n",
      " 4.2322607 4.2400074 4.2164507 4.302142  4.261593  4.34181   4.443276\n",
      " 4.3929167 4.339842  4.2573667 4.2685547 4.2888336 4.2657366 4.18053\n",
      " 4.1661205 4.2162457 4.147135  4.148626  4.187003  4.232201  4.2423687\n",
      " 4.2369614 4.2669554 4.254073  4.1822453 4.1556273 4.1377344 4.074727\n",
      " 3.97057   3.7726178 3.7671764 3.8814197 3.9332986 3.9721198 3.921308\n",
      " 3.8864765 3.8361602 3.8200266 3.9052675 3.8690608 3.850017  3.8077314\n",
      " 3.7798438 3.8476357 3.7912939 3.8133025 3.8224757 3.8341072 3.8631005\n",
      " 3.899923  3.8238235 3.7564206]\n",
      "\n",
      " D) Overall MSE: {lstm_mse:.4f}, R2: {lstm_r2:.4f}\n",
      "\n",
      " Final Output\n",
      "          DATE  Actual Interest Rate  RNN Predicted Interest Rate  \\\n",
      "0   2021-03-17                  1.63                     1.621728   \n",
      "1   2021-03-18                  1.71                     1.620811   \n",
      "2   2021-03-19                  1.74                     1.706077   \n",
      "3   2021-03-22                  1.69                     1.734784   \n",
      "4   2021-03-23                  1.63                     1.688533   \n",
      "..         ...                   ...                          ...   \n",
      "866 2024-08-29                  3.87                     3.817902   \n",
      "867 2024-08-30                  3.91                     3.847273   \n",
      "868 2024-09-03                  3.84                     3.888604   \n",
      "869 2024-09-04                  3.77                     3.816043   \n",
      "870 2024-09-05                  3.73                     3.745466   \n",
      "\n",
      "     LSTM Predicted Interest Rate  \n",
      "0                        1.616521  \n",
      "1                        1.626723  \n",
      "2                        1.688516  \n",
      "3                        1.722235  \n",
      "4                        1.691168  \n",
      "..                            ...  \n",
      "866                      3.834107  \n",
      "867                      3.863101  \n",
      "868                      3.899923  \n",
      "869                      3.823823  \n",
      "870                      3.756421  \n",
      "\n",
      "[871 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 6.\")\n",
    "print(\"RNN \\n A)Model Summary:\")\n",
    "rnn_model.summary()\n",
    "print(f\"\\n B)The number of epochs trained is 10 and the final training MSE loss is {rnn_final_mse:.4f}\")\n",
    "print(\"\\n C)Forecasted Values for Each Predicted Point:\")\n",
    "print(rnn_predictions)\n",
    "print(\"\\n D) Overall MSE: {rnn_mse:.4f}, R2: {rnn_r2:.4f}\")\n",
    "\n",
    "print(\"\\nLSTM \\n A)Model Summary:\")\n",
    "lstm_model.summary()\n",
    "print(f\"\\n B)The number of epochs trained is 10 and the final training MSE loss is {lstm_final_mse:.4f} \")\n",
    "print(\"\\n C)\")\n",
    "print(lstm_predictions)\n",
    "print(\"\\n D) Overall MSE: {lstm_mse:.4f}, R2: {lstm_r2:.4f}\")\n",
    "\n",
    "#creating 1 final dataframe that compares actual interests, RNN predictions and LSTM predictions side by side\n",
    "\n",
    "print(\"\\n Final Output\")\n",
    "test_dates = test_data['DATE'].iloc[seq_length:].reset_index(drop=True)\n",
    "test_interest_rates = test_data['IR'].iloc[seq_length:].reset_index(drop=True)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'DATE': test_dates,\n",
    "    'Actual Interest Rate': test_interest_rates,\n",
    "    'RNN Predicted Interest Rate': rnn_predictions,\n",
    "    'LSTM Predicted Interest Rate': lstm_predictions\n",
    "})\n",
    "\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
